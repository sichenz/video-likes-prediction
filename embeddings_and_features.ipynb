{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在运行此笔记本之前，请将变量 **VIDEO_FOLDER** 更改为您的视频文件夹名称，并将 **VIDEO_SHEET_CSV** 更改为您的 CSV 表，该表有每个视频 ID 到其对应的点赞数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffprobe is found in PATH.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting metadata with ffprobe: 100%|██████████| 13/13 [00:00<00:00, 19.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 'id' values:\n",
      "0    269\n",
      "1    282\n",
      "2    283\n",
      "3    268\n",
      "4    280\n",
      "Name: id, dtype: int64\n",
      "Metadata extraction and merging complete. Metadata saved to 'video_features_new.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the paths to the video folder and CSV files\n",
    "VIDEO_FOLDER = 'videos_new' #change name to your own video folder\n",
    "OUTPUT_CSV = 'video_features_new.csv'\n",
    "VIDEO_SHEET_CSV = 'video_sheet.csv'  #change name to your .csv sheet with video likes info\n",
    "\n",
    "def get_ffprobe_metadata(video_path):\n",
    "    \"\"\"\n",
    "    Extracts metadata from a video file using ffprobe.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing video metadata.\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    try:\n",
    "        # Construct ffprobe command\n",
    "        cmd = [\n",
    "            'ffprobe',\n",
    "            '-v', 'error',\n",
    "            '-print_format', 'json',\n",
    "            '-show_format',\n",
    "            '-show_streams',\n",
    "            video_path\n",
    "        ]\n",
    "\n",
    "        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "        # Parse the JSON output\n",
    "        ffprobe_output = json.loads(result.stdout)\n",
    "\n",
    "        # Extract general format info\n",
    "        format_info = ffprobe_output.get('format', {})\n",
    "        metadata['filename'] = os.path.basename(video_path)\n",
    "        metadata['duration_seconds'] = float(format_info.get('duration', 0))\n",
    "        metadata['file_size_MB'] = float(format_info.get('size', 0)) / (1024 * 1024)\n",
    "\n",
    "        # Initialize fields\n",
    "        metadata['fps'] = None\n",
    "        metadata['width'] = None\n",
    "        metadata['height'] = None\n",
    "        metadata['codec'] = None\n",
    "        metadata['has_audio'] = False\n",
    "        metadata['audio_fps'] = None\n",
    "        metadata['audio_channels'] = None\n",
    "        metadata['num_frames'] = 0\n",
    "\n",
    "        # Iterate over streams\n",
    "        streams = ffprobe_output.get('streams', [])\n",
    "        for stream in streams:\n",
    "            if stream.get('codec_type') == 'video':\n",
    "                metadata['codec'] = stream.get('codec_name', 'Unknown')\n",
    "                metadata['width'] = stream.get('width', 0)\n",
    "                metadata['height'] = stream.get('height', 0)\n",
    "                # Calculate fps\n",
    "                r_frame_rate = stream.get('r_frame_rate', '0/0')\n",
    "                nums = r_frame_rate.split('/')\n",
    "                if len(nums) == 2 and int(nums[1]) != 0:\n",
    "                    metadata['fps'] = float(nums[0]) / float(nums[1])\n",
    "                # Number of frames\n",
    "                if 'nb_frames' in stream:\n",
    "                    metadata['num_frames'] = int(stream['nb_frames'])\n",
    "            elif stream.get('codec_type') == 'audio':\n",
    "                metadata['has_audio'] = True\n",
    "                metadata['audio_fps'] = float(stream.get('sample_rate', 0))\n",
    "                metadata['audio_channels'] = int(stream.get('channels', 0))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {video_path}: {e}\")\n",
    "        metadata = {\n",
    "            'filename': os.path.basename(video_path),\n",
    "            'duration_seconds': None,\n",
    "            'fps': None,\n",
    "            'width': None,\n",
    "            'height': None,\n",
    "            'num_frames': None,\n",
    "            'file_size_MB': None,\n",
    "            'codec': None,\n",
    "            'has_audio': None,\n",
    "            'audio_fps': None,\n",
    "            'audio_channels': None\n",
    "        }\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def merge_video_metadata(df_metadata, info_csv):\n",
    "    \"\"\"\n",
    "    Merges the extracted metadata with additional video information.\n",
    "\n",
    "    Args:\n",
    "        df_metadata (pd.DataFrame): DataFrame containing video metadata.\n",
    "        info_csv (str): Path to 'video_info.csv'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Merged DataFrame.\n",
    "    \"\"\"\n",
    "    # Extract 'id' by removing the extension and converting to integer\n",
    "    try:\n",
    "        df_metadata['id'] = df_metadata['filename'].apply(lambda x: int(os.path.splitext(x)[0]))\n",
    "    except ValueError as ve:\n",
    "        print(\"Error extracting 'id' from filenames. Ensure all filenames before '.mp4' are integers.\")\n",
    "        print(ve)\n",
    "        # Optionally, handle or remove problematic rows\n",
    "        raise\n",
    "\n",
    "    # Debugging: Print some IDs to verify\n",
    "    print(\"Sample 'id' values:\")\n",
    "    print(df_metadata['id'].head())\n",
    "\n",
    "    # Drop existing 'title', 'publish_time', 'likes' columns if present\n",
    "    columns_to_drop = ['title', 'publish_time', 'likes']\n",
    "    df_metadata = df_metadata.drop(columns=[col for col in columns_to_drop if col in df_metadata.columns])\n",
    "\n",
    "    # Read the additional video info CSV\n",
    "    try:\n",
    "        df_info = pd.read_csv(info_csv)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{info_csv}' does not exist.\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame or handle as needed\n",
    "\n",
    "    # Rename relevant columns for clarity and consistency\n",
    "    df_info = df_info.rename(columns={\n",
    "        '序号': 'id',\n",
    "        '标题': 'title',\n",
    "        '发布时间': 'publish_time',\n",
    "        '点赞数': 'likes'\n",
    "    })\n",
    "\n",
    "    # Ensure 'id' in df_info is of integer type\n",
    "    df_info['id'] = df_info['id'].astype(int)\n",
    "\n",
    "    # Merge the DataFrames on 'id'\n",
    "    merged_df = pd.merge(\n",
    "        df_metadata,\n",
    "        df_info[['id', 'title', 'publish_time', 'likes']],\n",
    "        on='id',\n",
    "        how='left'  # Use left join to retain all metadata entries\n",
    "    )\n",
    "\n",
    "    # Check for any missing merges\n",
    "    missing_info = merged_df[merged_df['title'].isnull()]\n",
    "    if not missing_info.empty:\n",
    "        print(\"Warning: The following video IDs did not have matching entries in 'video_info.csv':\")\n",
    "        print(missing_info['id'].tolist())\n",
    "\n",
    "    # Drop the temporary 'id' column as it's no longer needed\n",
    "    merged_df = merged_df.drop(columns=['id'])\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "def main():\n",
    "    # Verify that ffprobe is accessible\n",
    "    from shutil import which\n",
    "    if which('ffprobe') is None:\n",
    "        print(\"Error: ffprobe is not installed or not found in PATH.\")\n",
    "        print(\"Please install ffmpeg (which includes ffprobe) and ensure it's accessible from the command line.\")\n",
    "        print(\"Refer to the installation instructions provided earlier.\")\n",
    "        return\n",
    "    else:\n",
    "        print(\"ffprobe is found in PATH.\")\n",
    "\n",
    "    # Get list of video files\n",
    "    try:\n",
    "        video_files = [f for f in os.listdir(VIDEO_FOLDER) if f.lower().endswith('.mp4')]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The directory '{VIDEO_FOLDER}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    if not video_files:\n",
    "        print(f\"No '.mp4' files found in the directory '{VIDEO_FOLDER}'.\")\n",
    "        return\n",
    "\n",
    "    video_paths = [os.path.join(VIDEO_FOLDER, f) for f in video_files]\n",
    "\n",
    "    # Initialize a list to store metadata dictionaries\n",
    "    metadata_list = []\n",
    "\n",
    "    # Iterate over each video and extract metadata\n",
    "    for video_path in tqdm(video_paths, desc=\"Extracting metadata with ffprobe\"):\n",
    "        metadata = get_ffprobe_metadata(video_path)\n",
    "        metadata_list.append(metadata)\n",
    "\n",
    "    # Convert the list of dictionaries to a pandas DataFrame\n",
    "    df_metadata = pd.DataFrame(metadata_list)\n",
    "\n",
    "    # Reorder columns for better readability\n",
    "    columns_order = [\n",
    "        'filename',\n",
    "        'duration_seconds',\n",
    "        'fps',\n",
    "        'width',\n",
    "        'height',\n",
    "        'num_frames',\n",
    "        'file_size_MB',\n",
    "        'codec',\n",
    "        'has_audio',\n",
    "        'audio_fps',\n",
    "        'audio_channels'\n",
    "    ]\n",
    "    \n",
    "    # Ensure all columns are present before reordering\n",
    "    df_metadata = df_metadata.reindex(columns=columns_order)\n",
    "\n",
    "    # ================================\n",
    "    # Merge with Additional Video Info\n",
    "    # ================================\n",
    "\n",
    "    # Define the path to the additional CSV file\n",
    "    # VIDEO_SHEET_CSV = 'video_info.csv'  # Already defined at the top\n",
    "\n",
    "    # Check if the additional CSV file exists\n",
    "    if not os.path.exists(VIDEO_SHEET_CSV):\n",
    "        print(f\"Error: The file '{VIDEO_SHEET_CSV}' does not exist.\")\n",
    "        print(\"Please ensure the additional CSV file is present in the working directory.\")\n",
    "        return\n",
    "\n",
    "    # Perform the merge\n",
    "    try:\n",
    "        merged_df = merge_video_metadata(df_metadata, VIDEO_SHEET_CSV)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during merging: {e}\")\n",
    "        return\n",
    "\n",
    "    if merged_df.empty:\n",
    "        print(\"Merged DataFrame is empty. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Reorder columns to include the new fields\n",
    "    final_columns_order = [\n",
    "        'filename',\n",
    "        'duration_seconds',\n",
    "        'fps',\n",
    "        'width',\n",
    "        'height',\n",
    "        'num_frames',\n",
    "        'file_size_MB',\n",
    "        'codec',\n",
    "        'has_audio',\n",
    "        'audio_fps',\n",
    "        'audio_channels',\n",
    "        'title',\n",
    "        'publish_time',\n",
    "        'likes'\n",
    "    ]\n",
    "    \n",
    "    # Ensure all final columns are present\n",
    "    missing_final_cols = [col for col in final_columns_order if col not in merged_df.columns]\n",
    "    if missing_final_cols:\n",
    "        print(f\"Warning: The following expected columns are missing in the merged DataFrame: {missing_final_cols}\")\n",
    "        # Optionally, handle missing columns, e.g., fill with NaN\n",
    "        for col in missing_final_cols:\n",
    "            merged_df[col] = pd.NA\n",
    "\n",
    "    merged_df = merged_df[final_columns_order]\n",
    "\n",
    "    # Save the merged DataFrame to a CSV file\n",
    "    merged_df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Metadata extraction and merging complete. Metadata saved to '{OUTPUT_CSV}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb3682f2aed488b9303105ae8c043fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing videos and extracting embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3c317cef9544ce95cd3e21744bc5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Embedding Videos:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to video_embeddings_new.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "import av\n",
    "from transformers import VideoLlavaForConditionalGeneration, VideoLlavaProcessor\n",
    "from huggingface_hub import hf_hub_download\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    \"\"\"\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (av.container.input.InputContainer): PyAV container.\n",
    "        indices (List[int]): List of frame indices to decode.\n",
    "    Returns:\n",
    "        np.ndarray: Decoded frames of shape (num_frames, height, width, 3).\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    if len(frames) < len(indices):\n",
    "        # Handle cases where the video has fewer frames than expected\n",
    "        last_frame = frames[-1]\n",
    "        while len(frames) < len(indices):\n",
    "            frames.append(last_frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "def extract_video_embedding(model, processor, video_path, device):\n",
    "    \"\"\"\n",
    "    Extract embedding for a single video.\n",
    "    Args:\n",
    "        model (VideoLlavaForConditionalGeneration): The Video-LLaVA model.\n",
    "        processor (VideoLlavaProcessor): The processor for Video-LLaVA.\n",
    "        video_path (str): Path to the video file.\n",
    "        device (torch.device): Device to run the model on.\n",
    "    Returns:\n",
    "        np.ndarray: Embedding vector for the video.\n",
    "    \"\"\"\n",
    "    container = av.open(video_path)\n",
    "    total_frames = container.streams.video[0].frames\n",
    "    if total_frames == 0:\n",
    "        raise ValueError(f\"No frames found in video: {video_path}\")\n",
    "    \n",
    "    # Sample uniformly 8 frames\n",
    "    step = max(total_frames // 8, 1)\n",
    "    indices = list(range(0, min(total_frames, step * 8), step))[:8]\n",
    "    if len(indices) < 8:\n",
    "        # Pad with the last frame if not enough frames\n",
    "        indices += [indices[-1]] * (8 - len(indices))\n",
    "    video = read_video_pyav(container, indices)\n",
    "\n",
    "    # Prepare prompt (dummy prompt as we are extracting embeddings)\n",
    "    prompt = \"USER: <video>\\nExtract embedding. ASSISTANT:\"\n",
    "    \n",
    "    # Important: The processor will handle the 'patch_size' and 'vision_feature_select_strategy'\n",
    "    inputs = processor(text=prompt, videos=video, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "        # Extract video_hidden_states\n",
    "        video_hidden_states = outputs.video_hidden_states  # Shape: (batch_size, num_frames, hidden_size)\n",
    "        # Average across frames to get a single embedding vector\n",
    "        embedding = video_hidden_states.mean(dim=1).squeeze().cpu().numpy()\n",
    "    return embedding\n",
    "\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize model and processor\n",
    "    model_name = \"LanguageBind/Video-LLaVA-7B-hf\"\n",
    "    model = VideoLlavaForConditionalGeneration.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\" if device.type == \"cuda\" else None\n",
    "    )\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Load the processor\n",
    "    processor = VideoLlavaProcessor.from_pretrained(model_name)\n",
    "\n",
    "    # -- Fix for deprecation warning: set patch_size and vision_feature_select_strategy --\n",
    "    # You can explicitly set them here. The values shown (14, \"mean\") are just examples.\n",
    "    # Adjust them if your model is configured differently.\n",
    "    processor.patch_size = 14\n",
    "    processor.vision_feature_select_strategy = \"mean\"\n",
    "    \n",
    "    # Set padding_side to \"left\" as per usage tips\n",
    "    processor.tokenizer.padding_side = \"left\"\n",
    "\n",
    "    # Directory containing videos\n",
    "    videos_dir = \"videos_new\"\n",
    "    if not os.path.isdir(videos_dir):\n",
    "        raise ValueError(f\"Directory '{videos_dir}' does not exist.\")\n",
    "\n",
    "    # Supported video extensions\n",
    "    video_extensions = {\".mp4\", \".mov\", \".avi\", \".mkv\"}\n",
    "\n",
    "    # List all video files\n",
    "    video_files = [\n",
    "        f for f in os.listdir(videos_dir)\n",
    "        if os.path.splitext(f)[1].lower() in video_extensions\n",
    "    ]\n",
    "\n",
    "    if not video_files:\n",
    "        raise ValueError(f\"No video files found in directory '{videos_dir}'.\")\n",
    "\n",
    "    # Prepare list to hold filename and embeddings\n",
    "    data = []\n",
    "\n",
    "    print(\"Processing videos and extracting embeddings...\")\n",
    "    for video_file in tqdm(video_files, desc=\"Embedding Videos\"):\n",
    "        video_path = os.path.join(videos_dir, video_file)\n",
    "        filename, _ = os.path.splitext(video_file)\n",
    "        try:\n",
    "            embedding = extract_video_embedding(model, processor, video_path, device)\n",
    "            # Convert embedding to list for CSV\n",
    "            embedding_list = embedding.tolist()\n",
    "            data.append({\n",
    "                \"filename\": filename,\n",
    "                \"embedding\": embedding_list\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {video_file}: {e}\")\n",
    "\n",
    "    # Convert data to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Save to CSV\n",
    "    output_csv = \"video_embeddings_new.csv\"\n",
    "    # To store embeddings as strings, join the list elements\n",
    "    df['embedding'] = df['embedding'].apply(lambda x: \",\".join(map(str, x)))\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Embeddings saved to {output_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge into one .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading video_embeddings_new.csv...\n",
      "Reading video_features_new.csv...\n",
      "Processing filenames to remove extensions...\n",
      "Merging DataFrames with merge type 'inner'...\n",
      "Saving merged data to video_info_new.csv...\n",
      "Merge completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "\n",
    "def merge_csv_files(embeddings_file, features_file, output_file, merge_type='inner'):\n",
    "    \"\"\"\n",
    "    Merges video_embeddings.csv with video_features.csv based on video filenames.\n",
    "\n",
    "    Args:\n",
    "        embeddings_file (str): Path to video_embeddings.csv.\n",
    "        features_file (str): Path to video_features.csv.\n",
    "        output_file (str): Path to save the merged video_info.csv.\n",
    "        merge_type (str): Type of merge to perform ('inner', 'outer', 'left', 'right').\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if input files exist\n",
    "    if not os.path.isfile(embeddings_file):\n",
    "        raise FileNotFoundError(f\"The file {embeddings_file} does not exist.\")\n",
    "    if not os.path.isfile(features_file):\n",
    "        raise FileNotFoundError(f\"The file {features_file} does not exist.\")\n",
    "\n",
    "    # Read video_embeddings.csv\n",
    "    print(f\"Reading {embeddings_file}...\")\n",
    "    embeddings_df = pd.read_csv(embeddings_file)\n",
    "\n",
    "    # Ensure necessary columns exist\n",
    "    if 'filename' not in embeddings_df.columns or 'embedding' not in embeddings_df.columns:\n",
    "        raise ValueError(f\"{embeddings_file} must contain 'filename' and 'embedding' columns.\")\n",
    "\n",
    "    # Convert 'filename' in embeddings_df to string\n",
    "    embeddings_df['filename'] = embeddings_df['filename'].astype(str)\n",
    "\n",
    "    # Read video_features.csv\n",
    "    print(f\"Reading {features_file}...\")\n",
    "    features_df = pd.read_csv(features_file)\n",
    "\n",
    "    # Ensure 'filename' column exists\n",
    "    if 'filename' not in features_df.columns:\n",
    "        raise ValueError(f\"{features_file} must contain a 'filename' column.\")\n",
    "\n",
    "    # Extract filename without extension from features_df\n",
    "    print(\"Processing filenames to remove extensions...\")\n",
    "    features_df['filename_no_ext'] = features_df['filename'].apply(lambda x: os.path.splitext(x)[0])\n",
    "\n",
    "    # Convert 'filename_no_ext' to string\n",
    "    features_df['filename_no_ext'] = features_df['filename_no_ext'].astype(str)\n",
    "\n",
    "    # Merge the two DataFrames on 'filename_no_ext' and 'filename'\n",
    "    print(f\"Merging DataFrames with merge type '{merge_type}'...\")\n",
    "    merged_df = pd.merge(\n",
    "        features_df,\n",
    "        embeddings_df,\n",
    "        left_on='filename_no_ext',\n",
    "        right_on='filename',\n",
    "        how=merge_type\n",
    "    )\n",
    "\n",
    "    # Drop redundant columns\n",
    "    # After merging, 'filename_x' comes from features_df and 'filename_y' from embeddings_df\n",
    "    columns_to_drop = ['filename_no_ext', 'filename_y']\n",
    "    merged_df.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "    # Rename 'filename_x' to 'filename'\n",
    "    merged_df.rename(columns={'filename_x': 'filename'}, inplace=True)\n",
    "\n",
    "    # Optional: Reorder columns to place 'embedding' at the end\n",
    "    cols = [col for col in merged_df.columns if col != 'embedding'] + ['embedding']\n",
    "    merged_df = merged_df[cols]\n",
    "\n",
    "    # Save the merged DataFrame to CSV\n",
    "    print(f\"Saving merged data to {output_file}...\")\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(\"Merge completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define file paths\n",
    "    embeddings_csv = 'video_embeddings_new.csv'  # Path to your video_embeddings.csv\n",
    "    features_csv = 'video_features_new.csv'      # Path to your video_features.csv\n",
    "    output_csv = 'video_info_new.csv'            # Desired output file\n",
    "\n",
    "    # Define merge type: 'inner', 'outer', 'left', 'right'\n",
    "    # 'inner' will only include rows with matching filenames in both CSVs\n",
    "    # 'outer' will include all rows, filling NaN where there are no matches\n",
    "    merge_type = 'inner'  # Change to 'outer' if you want all records\n",
    "\n",
    "    # Call the merge function\n",
    "    merge_csv_files(embeddings_csv, features_csv, output_csv, merge_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
